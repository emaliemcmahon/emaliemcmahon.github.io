---
---

@string{aps = {American Physical Society,}}

@article{MCMAHON2023B,
title = {Hierarchical organization of social action features along the lateral visual pathway},
journal = {Current Biology},
year = {2023},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2023.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S0960982223013738},
author = {Emalie McMahon and Michael F. Bonner and Leyla Isik},
keywords = {condition-rich fMRI, naturalistic perception, social action, social perception, encoding model, lateral visual stream, superior temporal sulcus},
abstract = {Recent theoretical work has argued that in addition to the classical ventral (what) and dorsal (where/how) visual streams, there is a third visual stream on the lateral surface of the brain specialized for processing social information. Like visual representations in the ventral and dorsal streams, representations in the lateral stream are thought to be hierarchically organized. However, no prior studies have comprehensively investigated the organization of naturalistic, social visual content in the lateral stream. To address this question, we curated a naturalistic stimulus set of 250 3-s videos of two people engaged in everyday actions. Each clip was richly annotated for its low-level visual features, mid-level scene and object properties, visual social primitives (including the distance between people and the extent to which they were facing), and high-level information about social interactions and affective content. Using a condition-rich fMRI experiment and a within-subject encoding model approach, we found that low-level visual features are represented in early visual cortex (EVC) and middle temporal (MT) area, mid-level visual social features in extrastriate body area (EBA) and lateral occipital complex (LOC), and high-level social interaction information along the superior temporal sulcus (STS). Communicative interactions, in particular, explained unique variance in regions of the STS after accounting for variance explained by all other labeled features. Taken together, these results provide support for representation of increasingly abstract social visual content—consistent with hierarchical organization—along the lateral visual stream and suggest that recognizing communicative actions may be a key computational goal of the lateral visual pathway.},
pdf = {McMahon2023_CurrentBiology.pdf}
}

@article{MCMAHON2023A,
title = {Seeing social interactions},
journal = {Trends in Cognitive Sciences},
year = {2023},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323002486},
author = {Emalie McMahon and Leyla Isik},
keywords = {social interactions, visual perception, cognitive neuroscience, computational cognitive science},
abstract = {Seeing the interactions between other people is a critical part of our everyday visual experience, but recognizing the social interactions of others is often considered outside the scope of vision and grouped with higher-level social cognition like theory of mind. Recent work, however, has revealed that recognition of social interactions is efficient and automatic, is well modeled by bottom-up computational algorithms, and occurs in visually-selective regions of the brain. We review recent evidence from these three methodologies (behavioral, computational, and neural) that converge to suggest the core of social interaction perception is visual. We propose a computational framework for how this process is carried out in the brain and offer directions for future interdisciplinary investigations of social perception.},
pdf = {McMahon2023_TiCS.pdf}
}

@misc{Lam2021,
title={Understanding Mental Representations Of Objects Through Verbs Applied To Them},
author={Ka Chun Lam and Francisco Pereira and Maryam Vaziri-Pashkam and Kristin Woodard and Emalie McMahon},
year={2021},
url={https://openreview.net/forum?id=tw60PTRSda2},
pdf = {Lam2021_CogSci.pdf},
abbr = {CogSci},
abstract = {In order to interact with objects in our environment, we rely on an understanding of the actions that can be performed on them, and the extent to which they rely or have an effect on the properties of the object. This knowledge is called the object “affordance”. We propose an approach for creating an embedding of objects in an affordance space, in which each dimension corresponds to an aspect of meaning shared by many actions, using text corpora. This embedding makes it possible to predict which verbs will be applicable to a given object, as captured in human judgments of affordance, better than a variety of alternative approaches. Furthermore, we show that the dimensions learned are interpretable, and that they correspond to typical patterns of interaction with objects. Finally, we show that the dimensions can be used to predict a state-of-the-art mental representation of objects, derived purely from human judgements of object similarity.}
}

@article{McMahon2021,
    author = {McMahon, Emalie and Kim, Daniel and Mehr, Samuel A. and Nakayama, Ken and Spelke, Elizabeth S. and Vaziri-Pashkam, Maryam},
    title = {The ability to predict actions of others from distributed cues is still developing in 6- to 8-year-old children},
    journal = {Journal of Vision},
    volume = {21},
    number = {5},
    pages = {14-14},
    year = {2021},
    month = {05},
    abstract = { Adults use distributed cues in the bodies of others to predict and counter their actions. To investigate the development of this ability, we had adults and 6- to 8-year-old children play a competitive game with a confederate who reached toward one of two targets. Child and adult participants, who sat across from the confederate, attempted to beat the confederate to the target by touching it before the confederate did. Adults used cues distributed through the head, shoulders, torso, and arms to predict the reaching actions. Children, in contrast, used cues in the arms and torso, but we did not find any evidence that they could use cues in the head or shoulders to predict the actions. These results provide evidence for a change in the ability to respond rapidly to predictive cues to others’ actions from childhood to adulthood. Despite humans’ sensitivity to action goals even in infancy, the ability to read cues from the body for action prediction in rapid interactive settings is still developing in children as old as 6 to 8 years of age. },
    issn = {1534-7362},
    doi = {10.1167/jov.21.5.14},
    url = {https://doi.org/10.1167/jov.21.5.14},
    pdf = {McMahon2021_JoV.pdf},
    abbr = {JoV},
    eprint = {https://arvojournals.org/arvo/content\_public/journal/jov/938529/i1534-7362-21-5-14\_1621070987.66637.pdf}
}

@article{McMahon2019,
    author = {McMahon, Emalie G. and Zheng, Charles Y. and Pereira, Francisco and Gonzalez, Ray and Ungerleider, Leslie G. and Vaziri-Pashkam, Maryam},
    title = {Subtle predictive movements reveal actions regardless of social context},
    journal = {Journal of Vision},
    volume = {19},
    number = {7},
    pages = {16-16},
    year = {2019},
    month = {07},
    abstract = { Humans have a remarkable ability to predict the actions of others. To address what information enables this prediction and how the information is modulated by social context, we used videos collected during an interactive reaching game. Two participants (an “initiator” and a “responder”) sat on either side of a plexiglass screen on which two targets were affixed. The initiator was directed to tap one of the two targets, and the responder had to either beat the initiator to the target (competition) or arrive at the same time (cooperation). In a psychophysics experiment, new observers predicted the direction of the initiators' reach from brief clips, which were clipped relative to when the initiator began reaching. A machine learning classifier performed the same task. Both humans and the classifier were able to determine the direction of movement before the finger lift-off in both social conditions. Further, using an information mapping technique, the relevant information was found to be distributed throughout the body of the initiator in both social conditions. Our results indicate that we reveal our intentions during cooperation, in which communicating the future course of actions is beneficial, and also during competition despite the social motivation to reveal less information.},
    issn = {1534-7362},
    doi = {10.1167/19.7.16},
    url = {https://doi.org/10.1167/19.7.16},
    pdf = {McMahon2019_JoV.pdf},
    abbr = {JoV},
    eprint = {https://arvojournals.org/arvo/content\_public/journal/jov/938093/i1534-7362-19-7-16.pdf}
}

@article {Corbetta2018,
      author = {Daniela Corbetta and Rebecca F. Wiener and Sabrina L. Thurman and Emalie McMahon},
      title = {The Embodied Origins of Infant Reaching: Implications for the Emergence of Eye-Hand Coordination},
      journal = {Kinesiology Review},
      year = {2018},
      publisher = {Human Kinetics},
      address = {Champaign IL, USA},
      volume = {7},
      number = {1},
      doi = {10.1123/kr.2017-0052},
      pages = {10 - 17},
      url = {https://journals.humankinetics.com/view/journals/krj/7/1/article-p10.xml},
      pdf = {Corbetta2018_KinesiologyReview.pdf},
      abbr = {Kin Rev},
      abstract = {This article reviews the literature on infant reaching, from past to present, to recount how our understanding of the emergence and development of this early goal-directed behavior has changed over the decades. We show that the still widely-accepted view, which considers the emergence and development of infant reaching as occurring primarily under the control of vision, is no longer sustainable. Increasing evidence suggests that the developmental origins of infant reaching is embodied. We discuss the implications of this alternative view for the development of eye-hand coordination and we propose a new scenario stressing the importance of the infant body-centered sensorimotor experiences in the months prior to the emergence of reaching as a possible critical step for the formation of eye-hand coordination. }

}
